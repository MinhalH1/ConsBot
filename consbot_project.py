# -*- coding: utf-8 -*-
"""Consbot project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zOjbesW3tI13KQbqBENlcsgCeSezFtqC
"""

pdf_path = '/content/PEC Documents.pdf'

# Setup Your Environment
!pip install PyMuPDF pdfplumber

#Import Libraries
import fitz  # PyMuPDF
import pdfplumber
import re

#Function to read a PDF file and extract text
def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

#Function to differentiate between scanned and digital PDFs:
def is_scanned_pdf(file_path):
    # Simple heuristic: Check for text extraction capability
    text = extract_text_from_pdf(file_path)
    return len(text.strip()) == 0

#Function to get page content:
def get_page_content(pdf_path, page_number):
    with pdfplumber.open(pdf_path) as pdf:
        page = pdf.pages[page_number - 1]  # page_number is 1-based
        return page.extract_text()

#Function to match documents:
def match_documents(pdf_path1, pdf_path2):
    text1 = extract_text_from_pdf(pdf_path1)
    text2 = extract_text_from_pdf(pdf_path2)
    return text1 == text2

#Extract text from pdf
text = extract_text_from_pdf(pdf_path)
print(text[:1000])  # Print the first 1000 characters for a quick check

#Find out the file is PDF or scanned
if is_scanned_pdf(pdf_path):
    print("Scanned PDF detected.")
else:
    print("Digital PDF detected.")

# To check the text from the page number
page_number = 23
page_content = get_page_content(pdf_path, page_number)
print(page_content)

import fitz  # PyMuPDF
import pdfplumber
from collections import Counter
import re

def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

def get_word_frequencies(text):
    words = re.findall(r'\b\w+\b', text.lower())
    return Counter(words)

def get_common_words(freq1, freq2):
    common_words = set(freq1.keys()).intersection(set(freq2.keys()))
    common_word_counts = {}
    for word in common_words:
        common_word_counts[word] = (freq1[word], freq2[word])
    return common_word_counts

def extract_headings_from_text(text):
    # Define a heuristic for headings; this can be adjusted based on actual formatting
    headings = re.findall(r'\b[A-Z][A-Z\s]+\b', text)  # Simple heuristic for uppercase headings
    return set(headings)

def get_common_headings(headings1, headings2):
    return headings1.intersection(headings2)

pdf_path1 = '/content/Goods-Supply-Document-05-10-2010-PEC.pdf'
pdf_path2 = '/content/Consultancy for smaller project.pdf'

text1 = extract_text_from_pdf(pdf_path1)
text2 = extract_text_from_pdf(pdf_path2)

# Get common words and their frequencies
freq1 = get_word_frequencies(text1)
freq2 = get_word_frequencies(text2)
common_words = get_common_words(freq1, freq2)

print("Common words and their frequencies in both documents:")
for word, (count1, count2) in common_words.items():
    print(f"'{word}': Document 1 = {count1}, Document 2 = {count2}")

# Get common headings
headings1 = extract_headings_from_text(text1)
headings2 = extract_headings_from_text(text2)
common_headings = get_common_headings(headings1, headings2)

print("\nCommon headings in both documents:")
for heading in common_headings:
    print(heading)

#common keywords in both documents
import pdfplumber
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

# Download NLTK resources (run once)
nltk.download('punkt')
nltk.download('stopwords')

def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

def get_keywords(text):
    words = word_tokenize(text.lower())
    words = [word for word in words if word.isalnum()]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    return Counter(words)

def compare_keywords(keywords1, keywords2):
    common_keywords = set(keywords1.keys()).intersection(set(keywords2.keys()))
    unique_to_doc1 = set(keywords1.keys()) - set(keywords2.keys())
    unique_to_doc2 = set(keywords2.keys()) - set(keywords1.keys())

    return {
        'common': common_keywords,
        'unique_to_doc1': unique_to_doc1,
        'unique_to_doc2': unique_to_doc2
    }

def print_keywords_summary(keywords, title):
    print(f"\n{title}:")
    for word, count in keywords.items():
        print(f"{word}: {count}")

# Paths to your PDF files
pdf_path1 = '/content/Consultancy for smaller project.pdf'
pdf_path2 = '/content/PEC Documents.pdf'

# Extract text from PDFs
text1 = extract_text_from_pdf(pdf_path1)
text2 = extract_text_from_pdf(pdf_path2)

# Get keywords from text
keywords1 = get_keywords(text1)
keywords2 = get_keywords(text2)

# Print summaries of keywords
print_keywords_summary(keywords1, 'Keywords in Document 1')
print_keywords_summary(keywords2, 'Keywords in Document 2')

# Compare keywords
comparison = compare_keywords(keywords1, keywords2)

print("\nCommon Keywords:")
for word in comparison['common']:
    print(word)

print("\nKeywords Unique to Document 1:")
for word in comparison['unique_to_doc1']:
    print(word)

print("\nKeywords Unique to Document 2:")
for word in comparison['unique_to_doc2']:
    print(word)

# How many times word is repeated in the documents
import pdfplumber
import re
from collections import Counter

def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

def count_word_occurrences(text, word):
    # Convert text to lowercase to make the search case-insensitive
    text = text.lower()
    word = word.lower()

    # Use regex to find all occurrences of the word
    pattern = rf'\b{re.escape(word)}\b'
    occurrences = re.findall(pattern, text, re.IGNORECASE)

    return len(occurrences)

# Example usage
text = extract_text_from_pdf(pdf_path)

word_to_count = 'engineer'
word_count = count_word_occurrences(text, word_to_count)

print(f"The word '{word_to_count}' appears {word_count} times in the document.")

#To create a function that allows a user to find the whole paragraph in which a specific number of days
import pdfplumber
import re

def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

def find_paragraphs_with_days(text, days):
    # Define a regex pattern to find paragraphs with the specified number of days
    pattern = rf'([^.]*?\b{days}\s+days\b[^.]*\.)'

    # Find all paragraphs matching the pattern
    matches = re.findall(pattern, text, re.IGNORECASE)

    return matches

# Example usage
text = extract_text_from_pdf(pdf_path)

days_to_search = '28'  # For example, to find paragraphs mentioning "30 days"
paragraphs = find_paragraphs_with_days(text, days_to_search)

print(f"Paragraphs mentioning '{days_to_search} days':")
for paragraph in paragraphs:
    print(paragraph)
    print("\n" + "-"*80 + "\n")

# Search by words in the documents
import pdfplumber
import re

def extract_text_by_paragraphs(file_path):
    paragraphs = []
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            # Split the text into paragraphs based on newlines or other delimiters
            # Adjust the splitting based on your document's format
            para_list = re.split(r'\n+', text)
            for para in para_list:
                if para.strip():  # Avoid empty paragraphs
                    paragraphs.append(para.strip())
    return paragraphs

def count_word_occurrences(paragraphs, word):
    word = word.lower()
    word_count = 0
    paragraphs_with_word = []

    for para in paragraphs:
        para_lower = para.lower()
        # Count occurrences of the word in the paragraph
        occurrences = len(re.findall(r'\b{}\b'.format(re.escape(word)), para_lower))
        if occurrences > 0:
            word_count += occurrences
            paragraphs_with_word.append(para)

    return word_count, paragraphs_with_word

def display_results(word, total_count, paragraphs):
    print(f"The word '{word}' appears {total_count} times in the document.")
    print("\nParagraphs containing the word:")
    print("="*80)
    for i, para in enumerate(paragraphs, start=1):
        print(f"Paragraph {i}:")
        print(para)
        print("\n" + "-"*80 + "\n")

# Example usage
pdf_path = '/content/PEC Documents.pdf'
paragraphs = extract_text_by_paragraphs(pdf_path)

word_to_count = 'engineer'
word_count, paragraphs_with_word = count_word_occurrences(paragraphs, word_to_count)

# Display the results
display_results(word_to_count, word_count, paragraphs_with_word)